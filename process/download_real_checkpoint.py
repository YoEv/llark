#!/usr/bin/env python3
"""
ä¸‹è½½çœŸæ­£çš„ Llama-2-7b-chat-hf checkpoint
"""

import os
import sys
from pathlib import Path

def download_llama_checkpoint():
    """ä¸‹è½½ Llama-2-7b-chat-hf checkpoint"""
    
    try:
        from huggingface_hub import snapshot_download, login
        import torch
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("è¯·å®‰è£…: pip install huggingface_hub torch")
        return False
    
    # è®¾ç½®ä¸‹è½½è·¯å¾„
    base_path = Path("/home/evev/diversity-eval/external/llark/checkpoints/meta-llama")
    model_path = base_path / "Llama-2-7b-chat-hf"
    checkpoint_path = model_path / "checkpoint-100000"
    
    print(f"ğŸ¯ ç›®æ ‡è·¯å¾„: {model_path}")
    
    # åˆ›å»ºç›®å½•
    model_path.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ Hugging Face ç™»å½•
    print("ğŸ” æ£€æŸ¥ Hugging Face è®¿é—®æƒé™...")
    
    try:
        # å°è¯•ä¸‹è½½æ¨¡å‹
        print("ğŸš€ å¼€å§‹ä¸‹è½½ Llama-2-7b-chat-hf...")
        print("âš ï¸  è¿™å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼Œæ¨¡å‹å¤§å°çº¦ 13GB")
        
        downloaded_path = snapshot_download(
            repo_id="meta-llama/Llama-2-7b-chat-hf",
            local_dir=str(model_path),
            local_dir_use_symlinks=False,
            resume_download=True
        )
        
        print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {downloaded_path}")
        
        # å¦‚æœéœ€è¦ï¼Œåˆ›å»º checkpoint-100000 ç›®å½•çš„ç¬¦å·é“¾æ¥
        if not checkpoint_path.exists():
            print(f"ğŸ”— åˆ›å»º checkpoint é“¾æ¥: {checkpoint_path}")
            checkpoint_path.symlink_to(model_path, target_is_directory=True)
        
        # éªŒè¯å…³é”®æ–‡ä»¶
        required_files = [
            "config.json",
            "tokenizer_config.json", 
            "pytorch_model.bin.index.json",
            "tokenizer.model"
        ]
        
        print("\nğŸ“‹ éªŒè¯ä¸‹è½½çš„æ–‡ä»¶:")
        for file_name in required_files:
            file_path = model_path / file_name
            if file_path.exists():
                size = file_path.stat().st_size / (1024*1024)  # MB
                print(f"  âœ… {file_name} ({size:.1f} MB)")
            else:
                print(f"  âŒ {file_name} (ç¼ºå¤±)")
        
        print(f"\nğŸ‰ Llama-2-7b-chat-hf ä¸‹è½½å®Œæˆ!")
        print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        
        if "gated repo" in str(e).lower() or "access" in str(e).lower():
            print("\nğŸ” è¿™æ˜¯ä¸€ä¸ªå—é™æ¨¡å‹ï¼Œéœ€è¦:")
            print("1. åœ¨ https://huggingface.co/meta-llama/Llama-2-7b-chat-hf ç”³è¯·è®¿é—®æƒé™")
            print("2. ä½¿ç”¨ huggingface-cli login ç™»å½•")
            print("3. æˆ–è®¾ç½® HF_TOKEN ç¯å¢ƒå˜é‡")
            
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¦™ Llama-2-7b-chat-hf ä¸‹è½½å™¨")
    print("=" * 50)
    
    if download_llama_checkpoint():
        print("\nâœ… ä¸‹è½½æˆåŠŸ! ç°åœ¨å¯ä»¥ä½¿ç”¨çœŸæ­£çš„ checkpoint äº†")
    else:
        print("\nâŒ ä¸‹è½½å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨æœ€å°åŒ–æ¨¡å‹è¿›è¡Œæµ‹è¯•")
        print("ğŸ’¡ æˆ–è€…æ‰‹åŠ¨è®¾ç½® Hugging Face è®¿é—®æƒé™åé‡è¯•")

if __name__ == "__main__":
    main()