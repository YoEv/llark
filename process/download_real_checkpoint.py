#!/usr/bin/env python3
"""
ä¸‹è½½çœŸæ­£çš„ Llama-2-7b-chat-hf checkpoint
"""

import os
import sys
from pathlib import Path

def download_llama_checkpoint():
    """ä¸‹è½½ Llama-2-7b-chat-hf checkpoint"""
    
    try:
        from huggingface_hub import snapshot_download, login
        import torch
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("è¯·å®‰è£…: pip install huggingface_hub torch")
        return False
    
    # è®¾ç½®ä¸‹è½½è·¯å¾„ - ä½¿ç”¨scratchç›®å½•é¿å…ç£ç›˜é…é¢é™åˆ¶
    scratch_path = Path.home() / "scratch" / "llark_models"
    model_path = scratch_path / "meta-llama" / "Llama-2-7b-chat-hf"
    
    # åœ¨é¡¹ç›®ç›®å½•åˆ›å»ºç¬¦å·é“¾æ¥
    current_dir = Path(__file__).parent.parent  # external/llark/
    project_base_path = current_dir / "checkpoints" / "meta-llama"
    project_model_path = project_base_path / "Llama-2-7b-chat-hf"
    checkpoint_path = project_base_path / "checkpoint-100000"
    
    print(f"ğŸ¯ ç›®æ ‡è·¯å¾„: {model_path}")
    
    model_path.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥ Hugging Face token
    print("ğŸ” æ£€æŸ¥ Hugging Face è®¿é—®æƒé™...")
    
    # å°è¯•ä»ç¯å¢ƒå˜é‡è·å– token
    hf_token = os.environ.get('HF_TOKEN') or os.environ.get('HUGGING_FACE_HUB_TOKEN')
    
    if hf_token:
        print("âœ… æ‰¾åˆ° HF_TOKEN ç¯å¢ƒå˜é‡")
        try:
            login(token=hf_token)
            print("âœ… Hugging Face ç™»å½•æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸  ç™»å½•è­¦å‘Š: {e}")
    else:
        print("âš ï¸  æœªæ‰¾åˆ° HF_TOKEN ç¯å¢ƒå˜é‡")
        print("ğŸ’¡ å¦‚æœä¸‹è½½å¤±è´¥ï¼Œè¯·è®¾ç½®: export HF_TOKEN=your_token_here")
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨å®Œæ•´çš„æ¨¡å‹æ–‡ä»¶
    required_files = [
        "config.json",
        "tokenizer_config.json", 
        "pytorch_model.bin.index.json",
        "tokenizer.model",
        "pytorch_model-00001-of-00002.bin",
        "pytorch_model-00002-of-00002.bin"
    ]
    
    all_files_exist = True
    missing_files = []
    
    for file_name in required_files:
        file_path = model_path / file_name
        if not file_path.exists():
            all_files_exist = False
            missing_files.append(file_name)
        elif file_name.endswith('.bin') and file_path.stat().st_size < 1024*1024:  # æ£€æŸ¥.binæ–‡ä»¶å¤§å°è‡³å°‘1MB
            all_files_exist = False
            missing_files.append(f"{file_name} (æ–‡ä»¶ä¸å®Œæ•´)")
    
    if all_files_exist:
        print("âœ… å‘ç°å·²å­˜åœ¨çš„å®Œæ•´æ¨¡å‹æ–‡ä»¶ï¼Œè·³è¿‡ä¸‹è½½")
        print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
        return True
    else:
        print(f"âš ï¸  æ£€æµ‹åˆ°ç¼ºå¤±æˆ–ä¸å®Œæ•´çš„æ–‡ä»¶: {', '.join(missing_files)}")
        print("ğŸ”„ å°†é‡æ–°ä¸‹è½½æ¨¡å‹...")
    
    try:
        # å°è¯•ä¸‹è½½æ¨¡å‹
        print("ğŸš€ å¼€å§‹ä¸‹è½½ Llama-2-7b-chat-hf...")
        print("âš ï¸  è¿™å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼Œæ¨¡å‹å¤§å°çº¦ 13GB")
        
        downloaded_path = snapshot_download(
            repo_id="meta-llama/Llama-2-7b-chat-hf",
            local_dir=str(model_path),
            local_dir_use_symlinks=False,
            resume_download=True,
            token=hf_token  # ä¼ é€’ token
        )
        
        print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {downloaded_path}")
        
        # åˆ›å»ºé¡¹ç›®ç›®å½•åˆ°scratchç›®å½•çš„ç¬¦å·é“¾æ¥
        project_model_path.parent.mkdir(parents=True, exist_ok=True)
        if project_model_path.exists() and project_model_path.is_symlink():
            project_model_path.unlink()
        elif project_model_path.exists():
            import shutil
            shutil.rmtree(project_model_path)
            
        print(f"ğŸ”— åˆ›å»ºé¡¹ç›®é“¾æ¥: {project_model_path} -> {model_path}")
        project_model_path.symlink_to(model_path, target_is_directory=True)
        
        # å¦‚æœéœ€è¦ï¼Œåˆ›å»º checkpoint-100000 ç›®å½•çš„ç¬¦å·é“¾æ¥
        if not checkpoint_path.exists():
            print(f"ğŸ”— åˆ›å»º checkpoint é“¾æ¥: {checkpoint_path}")
            checkpoint_path.symlink_to(model_path, target_is_directory=True)
        
        # éªŒè¯å…³é”®æ–‡ä»¶
        required_files = [
            "config.json",
            "tokenizer_config.json", 
            "pytorch_model.bin.index.json",
            "tokenizer.model"
        ]
        
        print("\nğŸ“‹ éªŒè¯ä¸‹è½½çš„æ–‡ä»¶:")
        for file_name in required_files:
            file_path = model_path / file_name
            if file_path.exists():
                size = file_path.stat().st_size / (1024*1024)  # MB
                print(f"  âœ… {file_name} ({size:.1f} MB)")
            else:
                print(f"  âŒ {file_name} (ç¼ºå¤±)")
        
        print(f"\nğŸ‰ Llama-2-7b-chat-hf ä¸‹è½½å®Œæˆ!")
        print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        
        if "gated repo" in str(e).lower() or "access" in str(e).lower() or "401" in str(e):
            print("\nğŸ” è¿™æ˜¯ä¸€ä¸ªå—é™æ¨¡å‹ï¼Œéœ€è¦:")
            print("1. åœ¨ https://huggingface.co/meta-llama/Llama-2-7b-chat-hf ç”³è¯·è®¿é—®æƒé™")
            print("2. è·å–ä½ çš„ Hugging Face token:")
            print("   - è®¿é—® https://huggingface.co/settings/tokens")
            print("   - åˆ›å»ºä¸€ä¸ªæ–°çš„ token (éœ€è¦ 'Read' æƒé™)")
            print("3. è®¾ç½®ç¯å¢ƒå˜é‡:")
            print("   export HF_TOKEN=your_token_here")
            print("4. æˆ–è€…ä½¿ç”¨ huggingface-cli login å‘½ä»¤ç™»å½•")
            print("\nğŸ’¡ ä½ æåˆ°å·²ç»ç”³è¯·äº†è®¿é—®æƒé™ï¼Œç°åœ¨åªéœ€è¦è®¾ç½® token:")
            print("   export HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx")
            
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¦™ Llama-2-7b-chat-hf ä¸‹è½½å™¨")
    print("=" * 50)
    
    if download_llama_checkpoint():
        print("\nâœ… ä¸‹è½½æˆåŠŸ! ç°åœ¨å¯ä»¥ä½¿ç”¨çœŸæ­£çš„ checkpoint äº†")
    else:
        print("\nâŒ ä¸‹è½½å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨æœ€å°åŒ–æ¨¡å‹è¿›è¡Œæµ‹è¯•")
        print("ğŸ’¡ æˆ–è€…æ‰‹åŠ¨è®¾ç½® Hugging Face è®¿é—®æƒé™åé‡è¯•")

if __name__ == "__main__":
    main()