#!/usr/bin/env python3
"""
æœ€å°åŒ–æ¨¡å‹è®¾ç½®è„šæœ¬
åˆ›å»ºLLarkæ‰€éœ€çš„åŸºæœ¬æ–‡ä»¶ç»“æ„ï¼Œç”¨äºæµ‹è¯•ä»£ç ä¿®å¤
"""

import os
import json
import torch
from pathlib import Path

def create_minimal_model_structure():
    """åˆ›å»ºæœ€å°åŒ–çš„æ¨¡å‹æ–‡ä»¶ç»“æ„"""
    
    # ç›®æ ‡è·¯å¾„
    model_path = Path("/home/evev/diversity-eval/external/llark/checkpoints/meta-llama/Llama-2-7b-chat-hf/checkpoint-100000")
    
    print(f"ğŸ¯ åˆ›å»ºæ¨¡å‹ç›®å½•: {model_path}")
    model_path.mkdir(parents=True, exist_ok=True)
    
    # 1. åˆ›å»º config.json
    config = {
        "architectures": ["LlamaForCausalLM"],
        "bos_token_id": 1,
        "eos_token_id": 2,
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 11008,
        "max_position_embeddings": 4096,
        "model_type": "llama",
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "num_key_value_heads": 32,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-06,
        "rope_scaling": None,
        "rope_theta": 10000.0,
        "tie_word_embeddings": False,
        "torch_dtype": "float16",
        "transformers_version": "4.31.0",
        "use_cache": True,
        "vocab_size": 32000
    }
    
    config_path = model_path / "config.json"
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    print(f"âœ… åˆ›å»º config.json")
    
    # 2. åˆ›å»º tokenizer_config.json
    tokenizer_config = {
        "add_bos_token": True,
        "add_eos_token": False,
        "bos_token": "<s>",
        "eos_token": "</s>",
        "model_max_length": 4096,
        "pad_token": None,
        "sp_model_kwargs": {},
        "tokenizer_class": "LlamaTokenizer",
        "unk_token": "<unk>",
        "use_default_system_prompt": False
    }
    
    tokenizer_config_path = model_path / "tokenizer_config.json"
    with open(tokenizer_config_path, 'w') as f:
        json.dump(tokenizer_config, f, indent=2)
    print(f"âœ… åˆ›å»º tokenizer_config.json")
    
    # 3. åˆ›å»º trainer_state.json
    trainer_state = {
        "best_metric": None,
        "best_model_checkpoint": None,
        "epoch": 1.0,
        "global_step": 100000,
        "is_hyper_param_search": False,
        "is_local_process_zero": True,
        "is_world_process_zero": True,
        "log_history": [],
        "max_steps": 100000,
        "num_train_epochs": 1,
        "total_flos": 0,
        "trial_name": None,
        "trial_params": None
    }
    
    trainer_state_path = model_path / "trainer_state.json"
    with open(trainer_state_path, 'w') as f:
        json.dump(trainer_state, f, indent=2)
    print(f"âœ… åˆ›å»º trainer_state.json")
    
    # 4. åˆ›å»º training_args.bin (ç©ºæ–‡ä»¶)
    training_args_path = model_path / "training_args.bin"
    with open(training_args_path, 'wb') as f:
        f.write(b'')
    print(f"âœ… åˆ›å»º training_args.bin")
    
    # 5. åˆ›å»º pytorch_model.bin å ä½ç¬¦
    pytorch_model_path = model_path / "pytorch_model.bin"
    
    # åˆ›å»ºä¸€ä¸ªæœ€å°çš„å ä½ç¬¦æ¨¡å‹çŠ¶æ€å­—å…¸
    placeholder_state_dict = {
        'model.embed_tokens.weight': torch.randn(32000, 4096, dtype=torch.float16),
        'model.norm.weight': torch.ones(4096, dtype=torch.float16),
        'lm_head.weight': torch.randn(32000, 4096, dtype=torch.float16)
    }
    
    # æ·»åŠ ä¸€äº›å±‚çš„å ä½ç¬¦æƒé‡
    for i in range(2):  # åªåˆ›å»ºå‰2å±‚ä½œä¸ºç¤ºä¾‹
        placeholder_state_dict.update({
            f'model.layers.{i}.self_attn.q_proj.weight': torch.randn(4096, 4096, dtype=torch.float16),
            f'model.layers.{i}.self_attn.k_proj.weight': torch.randn(4096, 4096, dtype=torch.float16),
            f'model.layers.{i}.self_attn.v_proj.weight': torch.randn(4096, 4096, dtype=torch.float16),
            f'model.layers.{i}.self_attn.o_proj.weight': torch.randn(4096, 4096, dtype=torch.float16),
            f'model.layers.{i}.mlp.gate_proj.weight': torch.randn(11008, 4096, dtype=torch.float16),
            f'model.layers.{i}.mlp.up_proj.weight': torch.randn(11008, 4096, dtype=torch.float16),
            f'model.layers.{i}.mlp.down_proj.weight': torch.randn(4096, 11008, dtype=torch.float16),
            f'model.layers.{i}.input_layernorm.weight': torch.ones(4096, dtype=torch.float16),
            f'model.layers.{i}.post_attention_layernorm.weight': torch.ones(4096, dtype=torch.float16),
        })
    
    torch.save(placeholder_state_dict, pytorch_model_path)
    print(f"âœ… åˆ›å»º pytorch_model.bin å ä½ç¬¦ ({pytorch_model_path.stat().st_size / 1024 / 1024:.1f} MB)")
    
    # 6. åˆ›å»º special_tokens_map.json
    special_tokens = {
        "bos_token": "<s>",
        "eos_token": "</s>",
        "unk_token": "<unk>"
    }
    
    special_tokens_path = model_path / "special_tokens_map.json"
    with open(special_tokens_path, 'w') as f:
        json.dump(special_tokens, f, indent=2)
    print(f"âœ… åˆ›å»º special_tokens_map.json")
    
    print(f"\nğŸ‰ æœ€å°åŒ–æ¨¡å‹ç»“æ„åˆ›å»ºå®Œæˆ!")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"ğŸ“‹ åˆ›å»ºçš„æ–‡ä»¶:")
    for file_path in model_path.iterdir():
        if file_path.is_file():
            size = file_path.stat().st_size
            if size > 1024 * 1024:
                size_str = f"{size / 1024 / 1024:.1f} MB"
            elif size > 1024:
                size_str = f"{size / 1024:.1f} KB"
            else:
                size_str = f"{size} B"
            print(f"   - {file_path.name} ({size_str})")
    
    print(f"\nâš ï¸  æ³¨æ„: pytorch_model.bin æ˜¯å ä½ç¬¦æ–‡ä»¶ï¼ŒåŒ…å«éšæœºæƒé‡")
    print(f"   å®é™…æ¨ç†éœ€è¦ä½¿ç”¨LLarkè®­ç»ƒåçš„æƒé‡")

if __name__ == "__main__":
    try:
        create_minimal_model_structure()
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()