#!/usr/bin/env python3
"""
å°†Metaå®˜æ–¹æ ¼å¼çš„Llamaæ¨¡å‹è½¬æ¢ä¸ºHugging Faceæ ¼å¼
"""
import os
import sys
import json
import shutil
from pathlib import Path

def convert_meta_to_hf():
    """è½¬æ¢Metaæ ¼å¼åˆ°HFæ ¼å¼"""
    
    # æºè·¯å¾„ï¼ˆMetaå®˜æ–¹æ ¼å¼ï¼‰
    meta_model_dir = "/home/evev/diversity-eval/external/llama-2-7b-chat"
    tokenizer_file = "/home/evev/diversity-eval/external/tokenizer.model"
    
    # ç›®æ ‡è·¯å¾„ï¼ˆLLarkæœŸæœ›çš„HFæ ¼å¼ï¼‰
    hf_model_dir = "/home/evev/diversity-eval/external/llark/checkpoints/meta-llama/Llama-2-7b-chat-hf"
    
    print(f"æºè·¯å¾„: {meta_model_dir}")
    print(f"ç›®æ ‡è·¯å¾„: {hf_model_dir}")
    
    # æ£€æŸ¥æºæ–‡ä»¶
    required_files = [
        os.path.join(meta_model_dir, "consolidated.00.pth"),
        os.path.join(meta_model_dir, "params.json"),
        tokenizer_file
    ]
    
    for file_path in required_files:
        if not os.path.exists(file_path):
            print(f"âŒ ç¼ºå°‘æ–‡ä»¶: {file_path}")
            return False
        else:
            print(f"âœ… æ‰¾åˆ°æ–‡ä»¶: {file_path}")
    
    # åˆ›å»ºç›®æ ‡ç›®å½•
    os.makedirs(hf_model_dir, exist_ok=True)
    print(f"âœ… åˆ›å»ºç›®æ ‡ç›®å½•: {hf_model_dir}")
    
    try:
        # æ£€æŸ¥transformersæ˜¯å¦å¯ç”¨
        try:
            import transformers
            print(f"\n=== æ£€æŸ¥ä¾èµ– ===")
            print(f"âœ… transformersç‰ˆæœ¬: {transformers.__version__}")
        except ImportError:
            print("âŒ æœªå®‰è£…transformersï¼Œå°è¯•å®‰è£…...")
            os.system("pip install transformers torch")
            
        # æ‰‹åŠ¨åˆ›å»ºHFæ ¼å¼
        print("\n=== æ‰‹åŠ¨åˆ›å»ºHFæ ¼å¼ ===")
        
        # å¤åˆ¶tokenizer
        shutil.copy2(tokenizer_file, os.path.join(hf_model_dir, "tokenizer.model"))
        print("âœ… å¤åˆ¶tokenizer.model")
        
        # è¯»å–params.jsonå¹¶è½¬æ¢ä¸ºconfig.json
        with open(os.path.join(meta_model_dir, "params.json"), 'r') as f:
            params = json.load(f)
        
        print(f"ğŸ“‹ åŸå§‹å‚æ•°: {params}")
        
        # åŸºäºå®é™…å‚æ•°åˆ›å»ºHFé…ç½®
        # Llama-2-7bçš„æ ‡å‡†é…ç½®
        hidden_size = params["dim"]  # 4096
        num_heads = params["n_heads"]  # 32
        num_layers = params["n_layers"]  # 32
        
        # è®¡ç®—intermediate_size (Llama-2-7bæ ‡å‡†å€¼æ˜¯11008)
        intermediate_size = int(hidden_size * 8 / 3)  # 10922.67
        multiple_of = params.get("multiple_of", 256)  # 256
        intermediate_size = ((intermediate_size + multiple_of - 1) // multiple_of) * multiple_of  # 11008
        
        print(f"ğŸ“‹ è®¡ç®—å¾—å‡º intermediate_size: {intermediate_size}")
        
        hf_config = {
            "architectures": ["LlamaForCausalLM"],
            "bos_token_id": 1,
            "eos_token_id": 2,
            "hidden_act": "silu",
            "hidden_size": hidden_size,
            "initializer_range": 0.02,
            "intermediate_size": intermediate_size,
            "max_position_embeddings": 4096,
            "model_type": "llama",
            "num_attention_heads": num_heads,
            "num_hidden_layers": num_layers,
            "num_key_value_heads": num_heads,  # Llama-2æ²¡æœ‰GQA
            "pretraining_tp": 1,
            "rms_norm_eps": params.get("norm_eps", 1e-6),
            "rope_scaling": None,
            "rope_theta": 10000.0,
            "tie_word_embeddings": False,
            "torch_dtype": "float16",
            "transformers_version": "4.35.0",
            "use_cache": True,
            "vocab_size": 32000  # Llama-2æ ‡å‡†è¯æ±‡è¡¨å¤§å°
        }
        
        with open(os.path.join(hf_model_dir, "config.json"), 'w') as f:
            json.dump(hf_config, f, indent=2)
        print("âœ… åˆ›å»ºconfig.json")
        
        # åˆ›å»ºtokenizeré…ç½®
        tokenizer_config = {
            "add_bos_token": True,
            "add_eos_token": False,
            "bos_token": "<s>",
            "eos_token": "</s>",
            "model_max_length": 4096,
            "pad_token": None,
            "sp_model_kwargs": {},
            "tokenizer_class": "LlamaTokenizer",
            "unk_token": "<unk>"
        }
        
        with open(os.path.join(hf_model_dir, "tokenizer_config.json"), 'w') as f:
            json.dump(tokenizer_config, f, indent=2)
        print("âœ… åˆ›å»ºtokenizer_config.json")
        
        # åˆ›å»ºgeneration_config.json
        generation_config = {
            "bos_token_id": 1,
            "do_sample": True,
            "eos_token_id": 2,
            "max_length": 4096,
            "pad_token_id": 0,
            "temperature": 0.6,
            "top_p": 0.9
        }
        
        with open(os.path.join(hf_model_dir, "generation_config.json"), 'w') as f:
            json.dump(generation_config, f, indent=2)
        print("âœ… åˆ›å»ºgeneration_config.json")
        
        # åˆ›å»ºè½¯é“¾æ¥æŒ‡å‘åŸå§‹æƒé‡æ–‡ä»¶
        weight_link = os.path.join(hf_model_dir, "pytorch_model.bin")
        if os.path.exists(weight_link):
            os.remove(weight_link)
        
        os.symlink(
            os.path.join(meta_model_dir, "consolidated.00.pth"),
            weight_link
        )
        print("âœ… åˆ›å»ºæƒé‡æ–‡ä»¶è½¯é“¾æ¥")
        
        print(f"\nâœ… è½¬æ¢å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {hf_model_dir}")
        
        # åˆ—å‡ºè½¬æ¢åçš„æ–‡ä»¶
        print("\nè½¬æ¢åçš„æ–‡ä»¶:")
        for file in sorted(os.listdir(hf_model_dir)):
            file_path = os.path.join(hf_model_dir, file)
            if os.path.islink(file_path):
                target = os.readlink(file_path)
                size = os.path.getsize(target) if os.path.exists(target) else 0
                print(f"  {file} -> {target} ({size:,} bytes)")
            else:
                size = os.path.getsize(file_path)
                print(f"  {file} ({size:,} bytes)")
        
        return True
        
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_model_loading():
    """æµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½æ­£ç¡®åŠ è½½"""
    hf_model_dir = "/home/evev/diversity-eval/external/llark/checkpoints/meta-llama/Llama-2-7b-chat-hf"
    
    print(f"\n=== æµ‹è¯•æ¨¡å‹åŠ è½½ ===")
    
    try:
        from transformers import LlamaTokenizer, LlamaForCausalLM
        
        print("å°è¯•åŠ è½½tokenizer...")
        tokenizer = LlamaTokenizer.from_pretrained(hf_model_dir)
        print("âœ… tokenizeråŠ è½½æˆåŠŸ")
        
        print("å°è¯•åŠ è½½æ¨¡å‹é…ç½®...")
        from transformers import LlamaConfig
        config = LlamaConfig.from_pretrained(hf_model_dir)
        print("âœ… æ¨¡å‹é…ç½®åŠ è½½æˆåŠŸ")
        
        print("âœ… æ¨¡å‹æ ¼å¼éªŒè¯é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        print("è¿™å¯èƒ½æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºæƒé‡æ ¼å¼å¯èƒ½éœ€è¦è¿›ä¸€æ­¥è½¬æ¢")
        return False


if __name__ == "__main__":
    print("=== Meta Llama è½¬ Hugging Face æ ¼å¼ ===")
    
    if convert_meta_to_hf():
        print("\nè½¬æ¢æˆåŠŸï¼")
        test_model_loading()
    else:
        print("\nè½¬æ¢å¤±è´¥ï¼")
        sys.exit(1)
